# Paper Reading Journal ðŸ“š

This repository is a curated journal of research papers I have read in **machine learning theory, mathematics, and 
related areas**. 
Each entry distills the key contributions, mathematical foundations, and personal insights I gained 
from the paper. The goal is to maintain a structured record of my learning process which can also function as a reference
or a quick go-to guide for someone new in the field.

# Initialization
- LeCun, Bottou, Orr, MÃ¼ller (1998) â€“ Efficient BackProp: Early analysis of weight initialization for neural networks.
Introduced fan-in scaling to maintain variance in activations.

- Glorot & Bengio (2010) â€“ Understanding the Difficulty of Training Deep Feedforward Neural Networks: Introduced Xavier/Glorot 
initialization.

- He, Zhang, Ren, Sun (2015) â€“ Delving Deep into Rectifiers: He/Kaiming initialization for ReLU activations. Solves 
exploding/vanishing gradient issues in deep CNNs.

- Saxe, McClelland, Ganguli (2014) â€“ Exact solutions to the nonlinear dynamics of learning in deep linear neural 
networks:Orthogonal initialization; preserves variance and gradient flow in very deep networks.

